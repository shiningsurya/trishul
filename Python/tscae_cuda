#!/usr/bin/env python3.7
import os
import numpy as np
from tqdm import tqdm
# trishul imports
import trishul.deeputils   as tdu
import trishul.deepmodels  as tdm
import trishul.deeptrainer as tdt
# torch imports
import torch       as t
import torch.nn    as tn
import torch.optim as to


def parse_args ():
    import argparse
    ap = argparse.ArgumentParser(prog='tscae', description='Trishul tool for Convolutional Autoencoder tasks.', epilog='Part of Trishul')
    add = ap.add_argument
    # add ('cnn', help='CNN model class.', choices=tdm.MODELS)
    # add ('it', help="Iteration index. defaults to latest.", default=None, type=int)
    # add ('params', help='Hyperparameters YAML file.', )
    add ('--root-dir', help='Data root directory.', default="/home/vlite-master/surya/triggers/", dest='rdir')
    add ('--model-dir', help="Model save directory.", default='/home/vlite-master/surya/triggers/Models/', dest='mdir')
    add ('--ds-dir', help="Dataset directory.", default='/home/vlite-master/surya/trishul/Datasets/', dest='dsir')
    return ap.parse_args ()

if __name__ == "__main__":
    args = parse_args ()
    print (args)
    # dataset
    IIDX = "i_wn"
    LIS  = "./injected_wn.dblist"
    DSet = tdu.Element (LIS)
    print (DSet.n)
    train_l, val_l   = DSet.train_test_split ()
    t_DataLoader     = tdu.DataLoader (DSet, sampler=train_l, batch_size=100, pin_memory=True, num_workers=2)
    v_DataLoader     = tdu.DataLoader (DSet, sampler=val_l, pin_memory=True, batch_size=10,num_workers=2)
    # cnn
    # CNN  = tdm.Byakugan (333)
    # CNN  = tdm.VGG11A (466).cuda()
    CAE  = tdm.GomuGomu (idx=550).cuda()
    ## load
    if False:
        IFILE = "/home/shining/mega/machine_learnings/hirahira/GomuGomu_i_wn_552_state.pth"
        tdu.LoadModel (IFILE, CAE)
    ## load
    CAE.idx = 553
    CAEID= "{0}_{1}_{2}".format(CAE.name, IIDX, CAE.idx)
    LFN  = tn.MSELoss (reduction='mean')
    OPM  = to.Adam (CAE.parameters(), lr=1e-3)
    # metrics
    # METS = {'recall':tdu.Recall_score, 'precision':tdu.Precision_score}
    METS = {}
    training_curve    = tdu.Curve (METS.keys(),epoch=70, type_="training")
    validation_curve  = tdu.Curve (METS.keys(),epoch=70,  type_="validation")
    # train
    num_epochs = 20
    EPC  = np.arange (num_epochs)
    tEPC = tqdm (EPC, desc="Epochs", unit="epochs", total=num_epochs)
    for i in tEPC:
        # training
        tmet = tdt.AETrainer (CAE, OPM, LFN, t_DataLoader, METS)
        training_curve (tmet)
        # validation
        vmet = tdt.AEEvaluator (CAE, LFN, v_DataLoader, METS)
        validation_curve (vmet)
        # print
        print ("Training   error = ", np.log10( np.mean(tmet['loss']) ))
        print ("Validation error = ", np.log10( np.mean(vmet['loss']) ))
    # save metrics
    curves = [training_curve, validation_curve]
    try:
        tdu.WritePickle (curves, os.path.join(args.mdir, "{0}_curves.pkl".format(CAEID)))
    except:
        print ("[EE] Writing pickle.")
    try:
        tdu.PlotCurve (curves, os.path.join(args.mdir, "{0}_curves.pdf".format(CAEID)))
    except:
        print ("[EE] Plotting learning curves.")
    # dump model
    try:
        tdu.DumpModel (os.path.join (args.mdir, "{0}_state".format(CAEID)), CAE, OPM)
    except:
        print ("[EE] Dumping model")


